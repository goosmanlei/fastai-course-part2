{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffEdit: 基于扩散模型的语义图像编辑\n",
    "\n",
    "**论文**: [DiffEdit: Diffusion-based Semantic Image Editing with Mask Guidance](https://arxiv.org/abs/2210.11427) (Couairon et al., 2022)\n",
    "\n",
    "## 核心思想\n",
    "\n",
    "DiffEdit 提出了一种**零样本**文本引导图像编辑方法，无需用户手动提供掩码。整个流程分为三步：\n",
    "\n",
    "1. **自动掩码生成 (Mask Generation)** — 对比 target prompt 和 reference prompt 条件下的噪声预测差异，自动识别需要编辑的区域\n",
    "2. **DDIM 编码/反转 (DDIM Inversion)** — 将源图像通过 DDIM 反转映射回噪声空间，保留图像信息\n",
    "3. **掩码 DDIM 解码 (Masked Decoding)** — 在去噪过程中，掩码内区域用目标文本引导去噪，掩码外区域保持编码时的 latent\n",
    "\n",
    "**优势**：自动掩码 + 背景保持，无需手动标注，无需训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数学公式\n",
    "\n",
    "### 1. 掩码生成\n",
    "\n",
    "给定源图像 $x_0$，添加噪声得到 $x_t$，分别用 target prompt $P_{tgt}$ 和 reference prompt $P_{ref}$ 预测噪声：\n",
    "\n",
    "$$M = \\text{Binarize}\\left( \\frac{1}{N} \\sum_{n=1}^{N} \\left| \\epsilon_\\theta(x_t^{(n)}, P_{tgt}) - \\epsilon_\\theta(x_t^{(n)}, P_{ref}) \\right| \\right)$$\n",
    "\n",
    "其中 $N$ 是采样次数，Binarize 是基于阈值的二值化操作。\n",
    "\n",
    "### 2. DDIM 采样（前向去噪）\n",
    "\n",
    "$$x_{t-1} = \\sqrt{\\bar\\alpha_{t-1}} \\underbrace{\\left( \\frac{x_t - \\sqrt{1-\\bar\\alpha_t} \\, \\epsilon_\\theta(x_t)}{\\sqrt{\\bar\\alpha_t}} \\right)}_{\\text{predicted } x_0} + \\sqrt{1 - \\bar\\alpha_{t-1}} \\cdot \\epsilon_\\theta(x_t)$$\n",
    "\n",
    "### 3. DDIM 反转（编码）\n",
    "\n",
    "将去噪公式反转，从 $x_t$ 推导 $x_{t+1}$：\n",
    "\n",
    "$$x_{t+1} = \\sqrt{\\bar\\alpha_{t+1}} \\underbrace{\\left( \\frac{x_t - \\sqrt{1-\\bar\\alpha_t} \\, \\epsilon_\\theta(x_t)}{\\sqrt{\\bar\\alpha_t}} \\right)}_{\\text{predicted } x_0} + \\sqrt{1 - \\bar\\alpha_{t+1}} \\cdot \\epsilon_\\theta(x_t)$$\n",
    "\n",
    "### 4. 掩码解码\n",
    "\n",
    "每个去噪步 $t$：\n",
    "\n",
    "$$\\hat{x}_{t-1} = M \\cdot \\text{denoise}(\\hat{x}_t, P_{tgt}) + (1-M) \\cdot x_{t-1}^{\\text{enc}}$$\n",
    "\n",
    "其中 $x_{t-1}^{\\text{enc}}$ 是 DDIM 反转过程中存储的对应时间步的 latent。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流水线示意图\n",
    "\n",
    "```\n",
    "源图像 x₀                 target: \"a zebra\"     reference: \"a horse\"\n",
    "   │                          │                      │\n",
    "   │                          ▼                      ▼\n",
    "   │                    ┌─────────────────────────────────┐\n",
    "   ├──(加噪)───────────►│  Step 1: 自动掩码生成            │\n",
    "   │                    │  对比两种文本下的噪声预测差异       │\n",
    "   │                    └──────────────┬──────────────────┘\n",
    "   │                                   │ Mask M\n",
    "   │                                   ▼\n",
    "   │                    ┌─────────────────────────────────┐\n",
    "   ├──(VAE编码)────────►│  Step 2: DDIM 反转 (编码)        │\n",
    "   │                    │  x₀ → x₁ → x₂ → ... → x_T      │\n",
    "   │                    │  存储所有中间 latent               │\n",
    "   │                    └──────────────┬──────────────────┘\n",
    "   │                                   │ {x_t} 序列\n",
    "   │                                   ▼\n",
    "   │                    ┌─────────────────────────────────┐\n",
    "   │                    │  Step 3: 掩码 DDIM 解码          │\n",
    "   │                    │  每步: mask内用target去噪          │\n",
    "   │                    │        mask外用编码latent替换       │\n",
    "   │                    └──────────────┬──────────────────┘\n",
    "   │                                   │\n",
    "   │                                   ▼\n",
    "   │                              编辑后图像\n",
    "   │                         (掩码内改变, 掩码外保持)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 第二部分：环境搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# 中文字体配置\n",
    "mpl.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # macOS 自带，支持中文\n",
    "mpl.rcParams['axes.unicode_minus'] = False               # 正常显示负号\n",
    "\n",
    "# 设备检测\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'使用设备: {device}')\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
    "\n",
    "# CLIP 文本编码器\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# UNet\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# DDIM Scheduler — 关键: clip_sample=False，否则反转不精确\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,        # 必须！不裁剪 pred_x0\n",
    "    set_alpha_to_one=False,\n",
    "    num_train_timesteps=1000,\n",
    ")\n",
    "\n",
    "print('模型加载完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具函数\n",
    "\n",
    "def text_enc(prompts, maxlen=None):\n",
    "    \"\"\"文本编码为 CLIP embeddings\"\"\"\n",
    "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n",
    "    return text_encoder(inp.input_ids.to(device))[0].half()\n",
    "\n",
    "def mk_img(t):\n",
    "    \"\"\"将张量转换为 PIL Image\"\"\"\n",
    "    image = (t/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray((image*255).round().astype(\"uint8\"))\n",
    "\n",
    "def encode_img(image, generator=None):\n",
    "    \"\"\"将 PIL Image 编码为 VAE latent\"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = image.resize((512, 512))\n",
    "        image = torch.tensor(np.array(image)).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "        image = (image * 2 - 1).to(device).half()\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(image).latent_dist.sample(generator) * 0.18215\n",
    "    return latent\n",
    "\n",
    "def decode_latents(latents):\n",
    "    \"\"\"将 VAE latent 解码为 PIL Image\"\"\"\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(1 / 0.18215 * latents).sample\n",
    "    return mk_img(image[0])\n",
    "\n",
    "def show_images(images, titles=None, figsize=None, suptitle=None):\n",
    "    \"\"\"并排显示多张图片\"\"\"\n",
    "    n = len(images)\n",
    "    if figsize is None: figsize = (4*n, 4)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1: axes = [axes]\n",
    "    for i, (ax, img) in enumerate(zip(axes, images)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if titles: ax.set_title(titles[i], fontsize=12)\n",
    "    if suptitle: fig.suptitle(suptitle, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 Stable Diffusion 生成一张马的测试图片作为源图\n",
    "\n",
    "def generate_image(prompt, num_steps=50, guidance_scale=7.5, seed=42):\n",
    "    \"\"\"使用 SD 从文本生成图片\"\"\"\n",
    "    generator = torch.manual_seed(seed)\n",
    "    \n",
    "    # 文本编码\n",
    "    text_emb = text_enc([prompt])\n",
    "    uncond_emb = text_enc([\"\"])\n",
    "    emb = torch.cat([uncond_emb, text_emb])\n",
    "    \n",
    "    # 初始化随机噪声\n",
    "    latents = torch.randn((1, 4, 64, 64), generator=generator, device=device, dtype=torch.float16)\n",
    "    \n",
    "    scheduler.set_timesteps(num_steps)\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    \n",
    "    for t in scheduler.timesteps:\n",
    "        latent_input = torch.cat([latents] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, t)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_input, t, encoder_hidden_states=emb).sample\n",
    "        \n",
    "        # CFG\n",
    "        noise_uncond, noise_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_uncond + guidance_scale * (noise_text - noise_uncond)\n",
    "        \n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    \n",
    "    return latents, decode_latents(latents)\n",
    "\n",
    "source_prompt = \"a photograph of a horse on a grass field, high quality, 4k\"\n",
    "source_latents, source_image = generate_image(source_prompt)\n",
    "show_images([source_image], titles=['源图像: 一匹马'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 第三部分：Step 1 — 自动掩码生成\n",
    "\n",
    "### 原理\n",
    "\n",
    "掩码生成的核心思想：**同一张有噪声的图片，在不同文本条件下预测的噪声是不同的**。\n",
    "\n",
    "- 如果图像区域与两个 prompt 的语义差异无关（比如背景草地），两种条件下预测的噪声相似\n",
    "- 如果图像区域是编辑目标（比如马 → 斑马），两种条件下预测的噪声差异很大\n",
    "\n",
    "通过多次采样取平均，可以得到稳定的差异图，再二值化为掩码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(latents, target_prompt, reference_prompt, num_samples=10,\n",
    "                  noise_level=0.5, threshold=0.5, num_inference_steps=50):\n",
    "    \"\"\"\n",
    "    DiffEdit Step 1: 自动掩码生成\n",
    "    \n",
    "    对源图像 latent 添加噪声，比较 target 和 reference prompt 下的噪声预测差异，\n",
    "    多次采样取平均后二值化得到掩码。\n",
    "    \n",
    "    Args:\n",
    "        latents: 源图像的 VAE latent [1, 4, 64, 64]\n",
    "        target_prompt: 目标文本 (如 \"a zebra\")\n",
    "        reference_prompt: 参考文本 (如 \"a horse\")\n",
    "        num_samples: 采样次数 N\n",
    "        noise_level: 噪声水平 (0~1)，对应 scheduler 的时间步比例\n",
    "        threshold: 二值化阈值\n",
    "        num_inference_steps: scheduler 步数\n",
    "    Returns:\n",
    "        mask: 二值掩码 [1, 1, 64, 64]\n",
    "        diff_map: 归一化差异图 [64, 64]\n",
    "    \"\"\"\n",
    "    # 文本编码\n",
    "    target_emb = text_enc([target_prompt])\n",
    "    reference_emb = text_enc([reference_prompt])\n",
    "    \n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # 选择噪声水平对应的时间步\n",
    "    t_idx = int(noise_level * num_inference_steps)\n",
    "    t_idx = min(t_idx, len(scheduler.timesteps) - 1)\n",
    "    t = scheduler.timesteps[-(t_idx + 1)]  # timesteps 是从大到小排列的\n",
    "    \n",
    "    diff_accumulator = torch.zeros(1, 4, 64, 64, device=device, dtype=torch.float32)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # 添加随机噪声\n",
    "        noise = torch.randn_like(latents)\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, t)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # target prompt 条件下的噪声预测\n",
    "            noise_pred_target = unet(noisy_latents, t, encoder_hidden_states=target_emb).sample\n",
    "            # reference prompt 条件下的噪声预测\n",
    "            noise_pred_ref = unet(noisy_latents, t, encoder_hidden_states=reference_emb).sample\n",
    "        \n",
    "        # 累加差异的绝对值\n",
    "        diff_accumulator += (noise_pred_target - noise_pred_ref).abs().float()\n",
    "    \n",
    "    # 平均\n",
    "    diff_avg = diff_accumulator / num_samples\n",
    "    \n",
    "    # 对通道取平均 → [1, 1, 64, 64]\n",
    "    diff_map = diff_avg.mean(dim=1, keepdim=True)\n",
    "    \n",
    "    # 归一化到 [0, 1]\n",
    "    diff_map_norm = (diff_map - diff_map.min()) / (diff_map.max() - diff_map.min() + 1e-8)\n",
    "    \n",
    "    # 二值化\n",
    "    mask = (diff_map_norm > threshold).float()\n",
    "    \n",
    "    return mask, diff_map_norm[0, 0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行掩码生成\n",
    "target_prompt = \"a photograph of a zebra on a grass field\"\n",
    "reference_prompt = \"a photograph of a horse on a grass field\"\n",
    "\n",
    "# 编码源图像\n",
    "source_latent = encode_img(source_image)\n",
    "\n",
    "mask, diff_map = generate_mask(\n",
    "    source_latent, \n",
    "    target_prompt, \n",
    "    reference_prompt,\n",
    "    num_samples=10,\n",
    "    noise_level=0.5,\n",
    "    threshold=0.5,\n",
    ")\n",
    "print(f'掩码形状: {mask.shape}, 掩码覆盖比例: {mask.mean().item():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化掩码\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "# 原图\n",
    "axes[0].imshow(source_image)\n",
    "axes[0].set_title('原图', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# 差异热力图\n",
    "im = axes[1].imshow(diff_map, cmap='hot', interpolation='nearest')\n",
    "axes[1].set_title('噪声预测差异热力图', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "\n",
    "# 二值掩码\n",
    "mask_np = mask[0, 0].cpu().numpy()\n",
    "axes[2].imshow(mask_np, cmap='gray', interpolation='nearest')\n",
    "axes[2].set_title('二值掩码 (阈值=0.5)', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "# 掩码叠加在原图上\n",
    "source_np = np.array(source_image.resize((64, 64)))\n",
    "overlay = source_np.copy().astype(float)\n",
    "mask_rgb = np.stack([mask_np * 255, mask_np * 50, mask_np * 50], axis=-1)\n",
    "overlay = overlay * 0.6 + mask_rgb * 0.4\n",
    "axes[3].imshow(overlay.clip(0, 255).astype(np.uint8))\n",
    "axes[3].set_title('掩码叠加原图', fontsize=12)\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.suptitle('Step 1: 自动掩码生成结果 (horse → zebra)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 阈值敏感性分析\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "fig, axes = plt.subplots(1, len(thresholds), figsize=(4*len(thresholds), 4))\n",
    "\n",
    "for ax, th in zip(axes, thresholds):\n",
    "    mask_th = (torch.tensor(diff_map) > th).float().numpy()\n",
    "    ax.imshow(mask_th, cmap='gray', interpolation='nearest')\n",
    "    coverage = mask_th.mean()\n",
    "    ax.set_title(f'阈值={th}\\n覆盖率={coverage:.1%}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('不同阈值下的掩码效果', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 第四部分：Step 2 — DDIM 编码（反转）\n",
    "\n",
    "### 原理\n",
    "\n",
    "DDIM 反转是 DDIM 采样的逆过程：\n",
    "\n",
    "- **标准 DDIM**: $x_t \\to x_{t-1}$（从噪声到干净图像）\n",
    "- **DDIM 反转**: $x_t \\to x_{t+1}$（从干净图像到噪声）\n",
    "\n",
    "反转公式:\n",
    "\n",
    "$$\\hat{x}_0 = \\frac{x_t - \\sqrt{1 - \\bar\\alpha_t} \\cdot \\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar\\alpha_t}}$$\n",
    "\n",
    "$$x_{t+1} = \\sqrt{\\bar\\alpha_{t+1}} \\cdot \\hat{x}_0 + \\sqrt{1 - \\bar\\alpha_{t+1}} \\cdot \\epsilon_\\theta(x_t, t)$$\n",
    "\n",
    "**关键**: 反转时使用 `guidance_scale=1.0`（不用 CFG），因为 CFG 会放大误差，导致反转不可逆。\n",
    "\n",
    "反转的目标是存储所有中间 latent $\\{x_0, x_1, ..., x_T\\}$，在 Step 3 中用于掩码外区域的保持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddim_inversion(latents, prompt, num_inference_steps=50, guidance_scale=1.0):\n",
    "    \"\"\"\n",
    "    DiffEdit Step 2: DDIM 反转 (编码)\n",
    "    \n",
    "    将源图像 latent 通过 DDIM 反转映射到噪声空间，\n",
    "    存储所有中间 latent 用于 Step 3 的掩码解码。\n",
    "    \n",
    "    Args:\n",
    "        latents: 源图像的 VAE latent [1, 4, 64, 64]\n",
    "        prompt: 文本提示（通常用源图像的描述）\n",
    "        num_inference_steps: 推理步数\n",
    "        guidance_scale: CFG 比例（反转时通常=1.0）\n",
    "    Returns:\n",
    "        all_latents: 所有中间 latent 的列表 [x_0, x_1, ..., x_T]\n",
    "    \"\"\"\n",
    "    # 文本编码\n",
    "    text_emb = text_enc([prompt])\n",
    "    uncond_emb = text_enc([\"\"])\n",
    "    \n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # 存储所有中间 latent（初始 latent 是 x_0）\n",
    "    all_latents = [latents.clone()]\n",
    "    \n",
    "    # 反转时间步：从小到大（scheduler.timesteps 是从大到小，需要反转）\n",
    "    reversed_timesteps = scheduler.timesteps.flip(0)\n",
    "    \n",
    "    for i, t in enumerate(reversed_timesteps):\n",
    "        # 当前 latent\n",
    "        x_t = latents\n",
    "        \n",
    "        if guidance_scale > 1.0:\n",
    "            # CFG: 同时预测有条件和无条件\n",
    "            latent_input = torch.cat([x_t] * 2)\n",
    "            emb = torch.cat([uncond_emb, text_emb])\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_input, t, encoder_hidden_states=emb).sample\n",
    "            noise_uncond, noise_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_uncond + guidance_scale * (noise_text - noise_uncond)\n",
    "        else:\n",
    "            # 无 CFG (guidance_scale=1.0)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(x_t, t, encoder_hidden_states=text_emb).sample\n",
    "        \n",
    "        # 获取 alpha 值\n",
    "        alpha_prod_t = scheduler.alphas_cumprod[t]\n",
    "        \n",
    "        # 计算下一个时间步\n",
    "        if i < len(reversed_timesteps) - 1:\n",
    "            t_next = reversed_timesteps[i + 1]\n",
    "            alpha_prod_t_next = scheduler.alphas_cumprod[t_next]\n",
    "        else:\n",
    "            # 最后一步，使用最大时间步的 alpha\n",
    "            alpha_prod_t_next = scheduler.alphas_cumprod[reversed_timesteps[-1]]\n",
    "        \n",
    "        # 预测 x_0\n",
    "        pred_x0 = (x_t - torch.sqrt(1 - alpha_prod_t) * noise_pred) / torch.sqrt(alpha_prod_t)\n",
    "        \n",
    "        # 反转公式: x_{t+1} = sqrt(alpha_{t+1}) * pred_x0 + sqrt(1 - alpha_{t+1}) * eps\n",
    "        latents = torch.sqrt(alpha_prod_t_next) * pred_x0 + torch.sqrt(1 - alpha_prod_t_next) * noise_pred\n",
    "        \n",
    "        all_latents.append(latents.clone())\n",
    "    \n",
    "    return all_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行 DDIM 反转\n",
    "num_steps = 50\n",
    "encode_ratio = 0.8  # 编码比率 r=0.8，只反转前 80% 的步数\n",
    "\n",
    "# 设定步数为完整步数，反转时从第0步到第 r*num_steps 步\n",
    "all_latents = ddim_inversion(source_latent, reference_prompt, num_inference_steps=num_steps)\n",
    "\n",
    "start_step = int(encode_ratio * num_steps)  # 从第 40 步开始解码\n",
    "print(f'反转完成! 共 {len(all_latents)} 个 latent (包括初始 x_0)')\n",
    "print(f'编码比率 r={encode_ratio}, 解码起始步: {start_step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证反转质量: 用 DDIM 前向去噪来恢复原图 (guidance_scale=1.0)\n",
    "def ddim_decode_simple(latents, prompt, num_inference_steps=50, guidance_scale=1.0):\n",
    "    \"\"\"简单的 DDIM 去噪（用于验证反转质量）\"\"\"\n",
    "    text_emb = text_enc([prompt])\n",
    "    uncond_emb = text_enc([\"\"])\n",
    "    \n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    for t in scheduler.timesteps:\n",
    "        if guidance_scale > 1.0:\n",
    "            latent_input = torch.cat([latents] * 2)\n",
    "            emb = torch.cat([uncond_emb, text_emb])\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_input, t, encoder_hidden_states=emb).sample\n",
    "            noise_uncond, noise_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_uncond + guidance_scale * (noise_text - noise_uncond)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latents, t, encoder_hidden_states=text_emb).sample\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    \n",
    "    return latents\n",
    "\n",
    "# 从最后一个反转 latent 开始去噪 (g=1.0)\n",
    "reconstructed_latent = ddim_decode_simple(all_latents[-1].clone(), reference_prompt, num_steps, guidance_scale=1.0)\n",
    "reconstructed_image = decode_latents(reconstructed_latent)\n",
    "\n",
    "show_images(\n",
    "    [source_image, reconstructed_image],\n",
    "    titles=['原图', 'DDIM 反转→去噪重建 (g=1.0)'],\n",
    "    figsize=(10, 4)\n",
    ")\n",
    "print('如果反转正确，两张图应该非常接近。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化反转过程：展示 5 个等距中间 latent 的解码效果\n",
    "num_show = 5\n",
    "indices = np.linspace(0, len(all_latents)-1, num_show, dtype=int)\n",
    "intermediate_imgs = [decode_latents(all_latents[idx]) for idx in indices]\n",
    "titles = [f'Step {idx}/{len(all_latents)-1}' for idx in indices]\n",
    "\n",
    "show_images(intermediate_imgs, titles=titles, figsize=(4*num_show, 4),\n",
    "            suptitle='DDIM 反转过程中的中间 latent 解码')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 第五部分：Step 3 — 掩码 DDIM 解码\n",
    "\n",
    "### 原理\n",
    "\n",
    "掩码解码在每个去噪步中执行：\n",
    "\n",
    "1. 用**目标文本** (target prompt) + CFG 进行标准去噪，得到 `denoised`\n",
    "2. 取 DDIM 反转中存储的对应时间步的编码 latent `encoded`\n",
    "3. 用掩码融合：\n",
    "   - **掩码内** (M=1)：使用去噪结果 → 被编辑为新内容\n",
    "   - **掩码外** (M=0)：使用编码 latent → 保持原始内容\n",
    "\n",
    "$$\\hat{x}_{t-1} = M \\cdot \\text{denoised}_{t-1} + (1-M) \\cdot x_{t-1}^{\\text{encoded}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffedit_decode(all_latents, mask, target_prompt, num_inference_steps=50,\n",
    "                    start_step=40, guidance_scale=7.5):\n",
    "    \"\"\"\n",
    "    DiffEdit Step 3: 掩码 DDIM 解码\n",
    "    \n",
    "    从反转的中间 latent 开始，用目标文本去噪，同时用掩码保持背景。\n",
    "    \n",
    "    Args:\n",
    "        all_latents: DDIM 反转得到的所有中间 latent [x_0, x_1, ..., x_T]\n",
    "        mask: 二值掩码 [1, 1, 64, 64]，1 表示要编辑的区域\n",
    "        target_prompt: 目标文本\n",
    "        num_inference_steps: 推理步数\n",
    "        start_step: 从第几步开始解码（对应编码比率）\n",
    "        guidance_scale: CFG 比例\n",
    "    Returns:\n",
    "        latents: 最终解码的 latent\n",
    "        intermediates: 中间结果列表（每 10 步存一张）\n",
    "    \"\"\"\n",
    "    # 文本编码\n",
    "    text_emb = text_enc([target_prompt])\n",
    "    uncond_emb = text_enc([\"\"])\n",
    "    emb = torch.cat([uncond_emb, text_emb])\n",
    "    \n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # 从反转的第 start_step 个 latent 开始\n",
    "    latents = all_latents[start_step].clone()\n",
    "    \n",
    "    # 只解码后 start_step 步\n",
    "    decode_timesteps = scheduler.timesteps[-start_step:]\n",
    "    \n",
    "    # 将掩码扩展到 4 通道以匹配 latent\n",
    "    mask_4ch = mask.expand(-1, 4, -1, -1).to(device).half()\n",
    "    \n",
    "    intermediates = []\n",
    "    \n",
    "    for i, t in enumerate(decode_timesteps):\n",
    "        # CFG 去噪\n",
    "        latent_input = torch.cat([latents] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, t)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_input, t, encoder_hidden_states=emb).sample\n",
    "        \n",
    "        noise_uncond, noise_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_uncond + guidance_scale * (noise_text - noise_uncond)\n",
    "        \n",
    "        # DDIM step\n",
    "        denoised = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        # 获取对应时间步的编码 latent\n",
    "        # decode_timesteps[i] 对应 all_latents 中的第 (start_step - 1 - i) 个\n",
    "        encoded_idx = start_step - 1 - i\n",
    "        encoded_latent = all_latents[encoded_idx]\n",
    "        \n",
    "        # 掩码融合: mask 内用去噪结果，mask 外用编码 latent\n",
    "        latents = mask_4ch * denoised + (1 - mask_4ch) * encoded_latent\n",
    "        \n",
    "        # 每 10 步存一张中间结果\n",
    "        if (i + 1) % 10 == 0 or i == len(decode_timesteps) - 1:\n",
    "            intermediates.append((i + 1, latents.clone()))\n",
    "    \n",
    "    return latents, intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行 DiffEdit 解码\n",
    "edited_latent, intermediates = diffedit_decode(\n",
    "    all_latents, mask,\n",
    "    target_prompt=target_prompt,\n",
    "    num_inference_steps=num_steps,\n",
    "    start_step=start_step,\n",
    "    guidance_scale=7.5\n",
    ")\n",
    "\n",
    "edited_image = decode_latents(edited_latent)\n",
    "print('DiffEdit 解码完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示最终结果: 原图、掩码、编辑结果\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(source_image)\n",
    "axes[0].set_title('原图 (horse)', fontsize=13)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask[0, 0].cpu().numpy(), cmap='gray', interpolation='nearest')\n",
    "axes[1].set_title('自动掩码', fontsize=13)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(edited_image)\n",
    "axes[2].set_title('DiffEdit 结果 (zebra)', fontsize=13)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('DiffEdit: horse → zebra', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码过程可视化\n",
    "intermediate_imgs = [decode_latents(lat) for step, lat in intermediates]\n",
    "titles = [f'Step {step}/{start_step}' for step, _ in intermediates]\n",
    "\n",
    "show_images(intermediate_imgs, titles=titles, figsize=(4*len(intermediates), 4),\n",
    "            suptitle='掩码 DDIM 解码过程')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 第六部分：完整管线与实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffedit(source_image, target_prompt, reference_prompt,\n",
    "             num_inference_steps=50, encode_ratio=0.8,\n",
    "             guidance_scale=7.5, mask_threshold=0.5,\n",
    "             num_mask_samples=10, noise_level=0.5):\n",
    "    \"\"\"\n",
    "    DiffEdit 完整管线：自动掩码生成 + DDIM 反转 + 掩码解码\n",
    "    \n",
    "    Args:\n",
    "        source_image: 源 PIL Image\n",
    "        target_prompt: 目标文本描述\n",
    "        reference_prompt: 参考文本描述（描述源图像）\n",
    "        num_inference_steps: 推理步数\n",
    "        encode_ratio: 编码比率 r (0~1)\n",
    "        guidance_scale: CFG 比例（解码时使用）\n",
    "        mask_threshold: 掩码二值化阈值\n",
    "        num_mask_samples: 掩码生成采样次数\n",
    "        noise_level: 掩码生成的噪声水平\n",
    "    Returns:\n",
    "        edited_image: 编辑后的 PIL Image\n",
    "        mask: 二值掩码\n",
    "        diff_map: 差异图\n",
    "    \"\"\"\n",
    "    print('Step 1: 生成掩码...')\n",
    "    source_latent = encode_img(source_image)\n",
    "    mask, diff_map = generate_mask(\n",
    "        source_latent, target_prompt, reference_prompt,\n",
    "        num_samples=num_mask_samples, noise_level=noise_level,\n",
    "        threshold=mask_threshold, num_inference_steps=num_inference_steps\n",
    "    )\n",
    "    print(f'  掩码覆盖率: {mask.mean().item():.2%}')\n",
    "    \n",
    "    print('Step 2: DDIM 反转...')\n",
    "    all_latents = ddim_inversion(source_latent, reference_prompt, num_inference_steps)\n",
    "    \n",
    "    start_step = int(encode_ratio * num_inference_steps)\n",
    "    print(f'  反转完成, 起始步: {start_step}')\n",
    "    \n",
    "    print('Step 3: 掩码解码...')\n",
    "    edited_latent, _ = diffedit_decode(\n",
    "        all_latents, mask, target_prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        start_step=start_step,\n",
    "        guidance_scale=guidance_scale\n",
    "    )\n",
    "    \n",
    "    edited_image = decode_latents(edited_latent)\n",
    "    print('完成！')\n",
    "    \n",
    "    return edited_image, mask, diff_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验 1: 马 → 斑马（主实验）\n",
    "edited_zebra, mask_zebra, diff_zebra = diffedit(\n",
    "    source_image,\n",
    "    target_prompt=\"a photograph of a zebra on a grass field\",\n",
    "    reference_prompt=\"a photograph of a horse on a grass field\",\n",
    "    encode_ratio=0.8,\n",
    "    guidance_scale=7.5,\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for ax, img, title in zip(axes, \n",
    "    [source_image, mask_zebra[0,0].cpu().numpy(), edited_zebra],\n",
    "    ['原图 (horse)', '自动掩码', '编辑结果 (zebra)']):\n",
    "    if isinstance(img, np.ndarray) and img.ndim == 2:\n",
    "        ax.imshow(img, cmap='gray')\n",
    "    else:\n",
    "        ax.imshow(img)\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('实验 1: horse → zebra', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验 2: 更多编辑示例\n",
    "\n",
    "# 先生成一张水果碗的源图\n",
    "_, fruit_image = generate_image(\"a photograph of a bowl of fruits on a table, high quality\", seed=123)\n",
    "\n",
    "# 水果碗 → 花碗\n",
    "edited_flowers, mask_flowers, _ = diffedit(\n",
    "    fruit_image,\n",
    "    target_prompt=\"a photograph of a bowl of flowers on a table\",\n",
    "    reference_prompt=\"a photograph of a bowl of fruits on a table\",\n",
    "    encode_ratio=0.8,\n",
    ")\n",
    "\n",
    "# 生成一张狗的源图\n",
    "_, dog_image = generate_image(\"a photograph of a dog sitting in a park, high quality\", seed=456)\n",
    "\n",
    "# 狗 → 猫\n",
    "edited_cat, mask_cat, _ = diffedit(\n",
    "    dog_image,\n",
    "    target_prompt=\"a photograph of a cat sitting in a park\",\n",
    "    reference_prompt=\"a photograph of a dog sitting in a park\",\n",
    "    encode_ratio=0.8,\n",
    ")\n",
    "\n",
    "# 展示结果\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for row, (src, msk, edited, label) in enumerate([\n",
    "    (fruit_image, mask_flowers, edited_flowers, 'fruits → flowers'),\n",
    "    (dog_image, mask_cat, edited_cat, 'dog → cat'),\n",
    "]):\n",
    "    axes[row, 0].imshow(src)\n",
    "    axes[row, 0].set_title('原图', fontsize=12)\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    axes[row, 1].imshow(msk[0, 0].cpu().numpy(), cmap='gray')\n",
    "    axes[row, 1].set_title('自动掩码', fontsize=12)\n",
    "    axes[row, 1].axis('off')\n",
    "    \n",
    "    axes[row, 2].imshow(edited)\n",
    "    axes[row, 2].set_title(f'编辑结果 ({label})', fontsize=12)\n",
    "    axes[row, 2].axis('off')\n",
    "\n",
    "plt.suptitle('实验 2: 更多编辑示例', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码比率消融\n",
    "\n",
    "编码比率 $r$ 控制从噪声空间的哪个位置开始解码：\n",
    "\n",
    "- **$r$ 小 (如 0.3)**：只反转到较低噪声水平，编辑能力弱，但背景保持好\n",
    "- **$r$ 大 (如 0.9)**：反转到较高噪声水平，编辑能力强，但背景可能有变化\n",
    "- **$r = 0.8$**：论文推荐的平衡点\n",
    "\n",
    "直觉：$r$ 越大，给去噪过程的\"自由度\"越大，能做出更大的改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码比率消融实验\n",
    "ratios = [0.3, 0.5, 0.7, 0.8, 0.9]\n",
    "ablation_images = []\n",
    "\n",
    "# 预先计算掩码和反转（只需做一次完整反转）\n",
    "source_latent_abl = encode_img(source_image)\n",
    "mask_abl, _ = generate_mask(\n",
    "    source_latent_abl, target_prompt, reference_prompt,\n",
    "    num_samples=10, noise_level=0.5, threshold=0.5\n",
    ")\n",
    "all_latents_abl = ddim_inversion(source_latent_abl, reference_prompt, num_steps)\n",
    "\n",
    "for r in ratios:\n",
    "    s = int(r * num_steps)\n",
    "    edited_lat, _ = diffedit_decode(\n",
    "        all_latents_abl, mask_abl, target_prompt,\n",
    "        num_inference_steps=num_steps, start_step=s, guidance_scale=7.5\n",
    "    )\n",
    "    ablation_images.append(decode_latents(edited_lat))\n",
    "    print(f'r={r} 完成')\n",
    "\n",
    "titles = [f'r={r}' for r in ratios]\n",
    "show_images(ablation_images, titles=titles, figsize=(4*len(ratios), 4),\n",
    "            suptitle='编码比率 r 消融实验 (horse → zebra)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比实验: DiffEdit vs 朴素 img2img\n",
    "\n",
    "def naive_img2img(source_image, target_prompt, num_inference_steps=50,\n",
    "                  encode_ratio=0.8, guidance_scale=7.5):\n",
    "    \"\"\"朴素 img2img: 直接加噪 + 去噪，无掩码保护\"\"\"\n",
    "    source_latent = encode_img(source_image)\n",
    "    \n",
    "    text_emb = text_enc([target_prompt])\n",
    "    uncond_emb = text_enc([\"\"])\n",
    "    emb = torch.cat([uncond_emb, text_emb])\n",
    "    \n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    # 直接加噪到对应的时间步\n",
    "    start_step = int(encode_ratio * num_inference_steps)\n",
    "    t_start = scheduler.timesteps[-start_step]\n",
    "    noise = torch.randn_like(source_latent)\n",
    "    latents = scheduler.add_noise(source_latent, noise, t_start)\n",
    "    \n",
    "    # 从 start_step 开始去噪\n",
    "    for t in scheduler.timesteps[-start_step:]:\n",
    "        latent_input = torch.cat([latents] * 2)\n",
    "        latent_input = scheduler.scale_model_input(latent_input, t)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_input, t, encoder_hidden_states=emb).sample\n",
    "        \n",
    "        noise_uncond, noise_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_uncond + guidance_scale * (noise_text - noise_uncond)\n",
    "        \n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    \n",
    "    return decode_latents(latents)\n",
    "\n",
    "# 运行朴素 img2img\n",
    "img2img_result = naive_img2img(source_image, target_prompt, encode_ratio=0.8)\n",
    "\n",
    "# 对比展示\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(source_image)\n",
    "axes[0].set_title('原图', fontsize=13)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img2img_result)\n",
    "axes[1].set_title('朴素 img2img\\n(无掩码，背景改变!)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(edited_zebra)\n",
    "axes[2].set_title('DiffEdit\\n(掩码保护，背景保持)', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('DiffEdit vs 朴素 img2img 对比', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 第七部分：总结\n",
    "\n",
    "### DiffEdit 三步法回顾\n",
    "\n",
    "| 步骤 | 操作 | 关键参数 | 作用 |\n",
    "|------|------|----------|------|\n",
    "| Step 1 | 自动掩码生成 | `num_samples=10`, `threshold=0.5` | 自动识别编辑区域 |\n",
    "| Step 2 | DDIM 反转 | `guidance_scale=1.0` | 保留源图信息 |\n",
    "| Step 3 | 掩码解码 | `encode_ratio=0.8`, `guidance_scale=7.5` | 执行编辑+保持背景 |\n",
    "\n",
    "### 优点\n",
    "- **零样本**: 无需训练或微调\n",
    "- **自动掩码**: 无需手动标注\n",
    "- **背景保持**: 掩码外区域完美保留\n",
    "- **灵活**: 通过编码比率控制编辑强度\n",
    "\n",
    "### 局限性\n",
    "- 掩码质量依赖 prompt 的准确性\n",
    "- DDIM 反转在高 CFG 下不精确\n",
    "- 编辑范围受限于简单的语义替换\n",
    "- 计算成本较高（反转 + 多次采样生成掩码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见问题\n",
    "\n",
    "**Q: 为什么 `clip_sample=False` 很重要？**\n",
    "\n",
    "A: 如果 `clip_sample=True`，scheduler 会裁剪 predicted $x_0$ 到 $[-1, 1]$，这在生成时是合理的（避免 latent 溢出），但在反转时会引入不可逆的信息损失，导致编码-解码不匹配。\n",
    "\n",
    "**Q: VAE 缩放因子 0.18215 是什么？**\n",
    "\n",
    "A: Stable Diffusion 训练时，VAE latent 被缩放使其标准差约为 1。0.18215 是训练集上统计得到的缩放因子。编码时乘以它，解码时除以它。\n",
    "\n",
    "**Q: 掩码分辨率为什么是 64×64？**\n",
    "\n",
    "A: 因为掩码在 latent 空间计算。VAE 的下采样倍率为 8x，所以 512×512 图像对应 64×64 latent。\n",
    "\n",
    "**Q: MPS (Apple Silicon) 兼容性？**\n",
    "\n",
    "A: MPS 支持基本推理，但可能在某些操作上有精度差异。如果结果异常，可以尝试将关键计算转到 float32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进阶: 掩码高斯模糊（平滑掩码边缘，减少编辑区域的突兀感）\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def smooth_mask(mask, sigma=2.0, threshold=0.5):\n",
    "    \"\"\"对掩码进行高斯模糊，使边缘更平滑\"\"\"\n",
    "    mask_np = mask[0, 0].cpu().numpy().astype(float)\n",
    "    smoothed = gaussian_filter(mask_np, sigma=sigma)\n",
    "    # 可选: 重新二值化（如果需要硬边缘）或保持软掩码\n",
    "    return torch.tensor(smoothed, device=device, dtype=torch.float16).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# 对比硬掩码 vs 软掩码\n",
    "soft_mask = smooth_mask(mask, sigma=2.0)\n",
    "\n",
    "# 用软掩码重新解码\n",
    "edited_soft, _ = diffedit_decode(\n",
    "    all_latents, soft_mask, target_prompt,\n",
    "    num_inference_steps=num_steps, start_step=start_step, guidance_scale=7.5\n",
    ")\n",
    "edited_soft_img = decode_latents(edited_soft)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(mask[0, 0].cpu().numpy(), cmap='gray')\n",
    "axes[0].set_title('硬掩码', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(soft_mask[0, 0].cpu().numpy(), cmap='gray')\n",
    "axes[1].set_title('软掩码 (σ=2.0)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(edited_zebra)\n",
    "axes[2].set_title('硬掩码编辑结果', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(edited_soft_img)\n",
    "axes[3].set_title('软掩码编辑结果', fontsize=12)\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.suptitle('硬掩码 vs 软掩码（高斯模糊）', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献\n",
    "\n",
    "1. **DiffEdit**: Couairon et al., \"DiffEdit: Diffusion-based Semantic Image Editing with Mask Guidance\", ICLR 2023. [arXiv:2210.11427](https://arxiv.org/abs/2210.11427)\n",
    "2. **DDIM**: Song et al., \"Denoising Diffusion Implicit Models\", ICLR 2021. [arXiv:2010.02502](https://arxiv.org/abs/2010.02502)\n",
    "3. **Latent Diffusion Models**: Rombach et al., \"High-Resolution Image Synthesis with Latent Diffusion Models\", CVPR 2022. [arXiv:2112.10752](https://arxiv.org/abs/2112.10752)\n",
    "4. **Classifier-Free Guidance**: Ho & Salimans, \"Classifier-Free Diffusion Guidance\", NeurIPS 2021 Workshop. [arXiv:2207.12598](https://arxiv.org/abs/2207.12598)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
