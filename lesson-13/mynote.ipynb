{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 13 - backprop: 手写反向传播 — 从裸函数到 nn.Module\n",
    "\n",
    "`03_backprop.ipynb` 的核心脉络：\n",
    "\n",
    "## 网络结构\n",
    "\n",
    "```\n",
    "inp [B,784] → lin1(w1,b1) → relu → lin2(w2,b2) → out [B,1] → MSE Loss\n",
    "```\n",
    "\n",
    "## 三个关键梯度公式（线性层 `out = inp @ w + b`）\n",
    "\n",
    "| 梯度 | 公式 | 形状 |\n",
    "|------|------|------|\n",
    "| `inp.g` | `out.g @ w.T` | [B, in] |\n",
    "| `w.g` | `inp.T @ out.g` | [in, out] |\n",
    "| `b.g` | `out.g.sum(0)` | [out] |\n",
    "\n",
    "直觉：前向 `inp @ w` 把信息投影到输出空间，反向 `out.g @ w.T` 把梯度投影回输入空间。\n",
    "\n",
    "## 代码演进（四个阶段）\n",
    "\n",
    "1. **裸函数** — `lin_grad()` + `forward_and_backward()` 手动串联\n",
    "2. **类封装** — `Lin / Relu / Mse` 各自持有 `__call__` + `backward`\n",
    "3. **Module 基类** — 抽取公共逻辑到 `Module.__call__` / `Module.backward`，子类只写 `forward` + `bwd`\n",
    "4. **PyTorch autograd** — `nn.Module` + `requires_grad_()` + `loss.backward()` 自动完成\n",
    "\n",
    "每个阶段都用 `test_close` 对比 PyTorch autograd 验证正确性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
