{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 12 - 01_matmul.ipynb: 从零实现矩阵乘法 — 逐步优化之旅\n",
    "\n",
    "本节课以 MNIST 数据集为载体，从最朴素的三重循环开始，逐步优化矩阵乘法的实现，最终达到约 **500万倍** 的加速。\n",
    "整个过程串联了 Python 基础、PyTorch tensor 操作、broadcasting 机制、Einstein 求和以及 CUDA 编程等核心概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 获取数据 & 基础工具\n",
    "\n",
    "- 下载 MNIST 手写数字数据集（`urlretrieve` + `gzip` + `pickle`）\n",
    "- 用纯 Python 的 `chunks` 生成器和 `itertools.islice` 将一维像素列表重塑为 28×28 图像\n",
    "- 手写一个简单的 `Matrix` 类，支持 `m[i, j]` 二维索引，体会 tensor 的本质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrix & Tensor\n",
    "\n",
    "- 将 numpy 数组转为 PyTorch `tensor`\n",
    "- `x_train.reshape((-1, 28, 28))` 得到图像张量\n",
    "- 理解 shape、type 等基本属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 随机数生成器\n",
    "\n",
    "- 手写 **Wichmann-Hill** 伪随机数生成算法（Python 2.3 之前使用的算法）\n",
    "- 用 `os.fork()` 演示子进程继承随机状态导致的安全隐患（父子进程产生相同随机数）\n",
    "- 对比纯 Python 随机数 vs `torch.randn` 的速度差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 矩阵乘法的逐步优化\n",
    "\n",
    "这是本课的核心——用多种方法实现 matmul，每一步都带来数量级的加速：\n",
    "\n",
    "### v0: 纯 Python 三重循环（~500ms）\n",
    "```python\n",
    "for i in range(ar):\n",
    "    for j in range(bc):\n",
    "        for k in range(ac):\n",
    "            c[i,j] += a[i,k] * b[k,j]\n",
    "```\n",
    "\n",
    "### v1: Numba JIT 加速内层循环\n",
    "- 用 `@njit` 编译 `dot` 函数，消除最内层 Python 循环\n",
    "- 仅剩两层 Python 循环\n",
    "\n",
    "### v2: 逐元素运算（Elementwise ops）\n",
    "- 利用 PyTorch 的向量化运算消除最内层循环：`c[i,j] = (a[i,:] * b[:,j]).sum()`\n",
    "- 或直接用 `torch.dot(a[i,:], b[:,j])`\n",
    "- 顺带介绍了 Frobenius 范数\n",
    "\n",
    "### v3: Broadcasting 版本（<0.1ms，~5000x 加速）\n",
    "- 利用 broadcasting 一次计算整行：`c[i] = (a[i,:,None] * b).sum(dim=0)`\n",
    "- 从三重循环减少到仅一层循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Broadcasting 详解\n",
    "\n",
    "这是理解上述优化的关键概念：\n",
    "\n",
    "- **标量广播**: `a > 0`，标量 0 被广播到与 a 相同的维度\n",
    "- **向量→矩阵广播**: `m + c`，向量 c 沿缺失维度广播\n",
    "- **`expand_as` & stride**: 广播不真正复制数据，而是将 stride 设为 0\n",
    "- **`unsqueeze` / `None` 索引**: 在指定位置插入维度 1\n",
    "- **广播规则**: 从尾部维度开始对齐，维度相等或其中一个为 1 即兼容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Einstein 求和 (einsum)\n",
    "\n",
    "- 用紧凑的下标记法表达乘积与求和：`torch.einsum('ik,kj->ij', a, b)`\n",
    "- 重复的字母（k）表示沿该轴相乘，输出中省略的字母表示沿该轴求和\n",
    "- 可以拆解为中间步骤理解：先 `'ik,kj->ikj'`（逐元素乘），再 `.sum(1)`（沿 k 求和）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PyTorch 内置运算\n",
    "\n",
    "- 直接使用 `torch.matmul` 或 `@` 运算符\n",
    "- 底层调用高度优化的 BLAS 库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CUDA 编程入门\n",
    "\n",
    "- 先用 Python 模拟 GPU kernel 的执行模型（grid + kernel 函数）\n",
    "- `launch_kernel` 模拟 GPU 的网格调度\n",
    "- 然后使用 `numba.cuda` 的 `@cuda.jit` 编写真正的 CUDA kernel\n",
    "- 关键概念：`cuda.grid(2)` 获取线程坐标、`blockspergrid`、`TPB`(threads per block)\n",
    "- 最终对比：`x_train.cuda() @ weights.cuda()` 利用 PyTorch 的 CUDA 后端\n",
    "\n",
    "**最终加速**：从纯 Python 的 ~500ms 到 CUDA 的 ~0.5ms，总计约 **500万倍** 提升！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结：优化路径一览\n",
    "\n",
    "| 版本 | 方法 | 关键技术 |\n",
    "|------|------|----------|\n",
    "| v0 | 三重 Python 循环 | 纯 Python |\n",
    "| v1 | Numba JIT dot | `@njit` 编译内层循环 |\n",
    "| v2 | 逐元素向量化 | `(a[i,:] * b[:,j]).sum()` / `torch.dot` |\n",
    "| v3 | Broadcasting | `(a[i,:,None] * b).sum(dim=0)` |\n",
    "| v4 | Einstein 求和 | `torch.einsum('ik,kj->ij', a, b)` |\n",
    "| v5 | PyTorch 内置 | `a @ b` / `torch.matmul` |\n",
    "| v6 | CUDA kernel | `numba.cuda` / PyTorch CUDA |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
